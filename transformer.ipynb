{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b336f0c8",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0f63d",
   "metadata": {},
   "source": [
    "In this notebook, we are going to define and implement a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b877da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a55cc6",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "<img src='assets/1/scaled-dot-product-3.PNG'/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeae70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention calculation \n",
    "# from the paper (section 3.2.1 Scaled Dot-Product Attention):\n",
    "# 'We compute the dot products of the query with all keys, divide each by √dk, \n",
    "# and apply a softmax function to obtain the weights on the values.'\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, mask=False):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.mask=mask\n",
    "        \n",
    "    def forward(self, Q, K, V):\n",
    "        \n",
    "        # compute dot product of the all query with all keys\n",
    "        dot_products = torch.matmul(Q, torch.transpose(K, -2, -1)) # transpose on last two dimensions\n",
    "        \n",
    "        # divide each by √dk\n",
    "        d_k = K.shape[-1]    # get length of key vector\n",
    "        scaled_dot_products = dot_products / np.sqrt(d_k)\n",
    "        \n",
    "        # apply a softmax function to obtain weights on values\n",
    "        weights = F.softmax(scaled_dot_products, dim=-1)\n",
    "        \n",
    "        # get weighted values by multiplying values with softmax scores\n",
    "        weighted_values = torch.matmul(weights, V)\n",
    "        \n",
    "        # apply mask to prevent positions from attending to subsequent positions in decoder\n",
    "        if self.mask==True:\n",
    "            size = weighted_values.shape\n",
    "            look_ahead_mask = torch.triu(torch.full(size, float('-inf')), diagonal=1)\n",
    "            look_ahead_mask[look_ahead_mask == 0] = 1\n",
    "            weighted_values = weighted_values * look_ahead_mask\n",
    "        \n",
    "        return weighted_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf76d2",
   "metadata": {},
   "source": [
    "Scaled Dot-product attention is identical to the [Dot-product attention algorithm](https://arxiv.org/pdf/1508.04025.pdf), except for the scaling  factor of 1/√dk.\n",
    ">For large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by 1/√dk. <sub>-from section 3.2.1 Scaled Dot-Product Attention<sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb23674",
   "metadata": {},
   "source": [
    "**Masked Multi-Head Attention**\n",
    "\n",
    "<img src='assets/1/masked-attention.PNG' width=50% height=50% /> \n",
    "\n",
    ">We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We **implement this inside of scaled dot-product attention** by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. <sub>-from section 3.2.3 Applications of Attention in our Model<sub>\n",
    "\n",
    ">We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. <sub>-from section 3.1 Encoder and Decoder Stacks<sub>\n",
    "    \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e318e1",
   "metadata": {},
   "source": [
    "Let's do a unit test on Scaled Dot-Product Attention implementation.\n",
    "\n",
    "<img src='assets/1/scaled-dot-product-matrix.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91b4c5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define data, n=3 embeddings(sequence length), ex: [Je, suis, etudiant]\n",
    "n = 3\n",
    "\n",
    "# from section 3.2.2 Multi-Head Attention\n",
    "# d_key = d_value = d_query\n",
    "d_key = 64 # key dimensionality\n",
    "\n",
    "# define K,Q and V matrices\n",
    "K = torch.randn(n, d_key)\n",
    "Q = torch.randn(n, d_key)\n",
    "V = torch.randn(n, d_key)\n",
    "\n",
    "scaled_dot_product = ScaledDotProductAttention()\n",
    "\n",
    "# apply scaled-dot product attention\n",
    "attention_scores = scaled_dot_product(Q, K, V)    \n",
    "\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test for scaled dot-prduct attention \n",
    "\n",
    "# each attention distribution should sum up to one\n",
    "\n",
    "print(torch.sum(attention_scores, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2541108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64])\n",
      "tensor([[-0.1329,     inf,    -inf,     inf,    -inf],\n",
      "        [-0.3348, -1.2286,    -inf,     inf,    -inf],\n",
      "        [-0.3494, -1.1837,  0.2570,     inf,    -inf]])\n"
     ]
    }
   ],
   "source": [
    "scaled_dot_product = ScaledDotProductAttention(mask=True)\n",
    "\n",
    "masked_output = scaled_dot_product(Q, K, V)\n",
    "print(masked_output.shape)\n",
    "\n",
    "print(masked_output[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6dca82",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "<img src='assets/1/attention-3.PNG'/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01083da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=512, d_key=64, mask=False):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # define Key, Query and Value weight matrices\n",
    "        self.WK = nn.Parameter(torch.randn(d_model, d_key))\n",
    "        self.WQ = nn.Parameter(torch.randn(d_model, d_key))\n",
    "        self.WV = nn.Parameter(torch.randn(d_model, d_key))\n",
    "        \n",
    "        # init scaled dot-product attention\n",
    "        self.scaled_dot_product = ScaledDotProductAttention(mask)\n",
    "        \n",
    "    def forward(self, inputs, encoder_output=None):\n",
    "        WK, WQ, WV = self.WK, self.WQ, self.WV\n",
    "        X = inputs if encoder_output is None else encoder_output\n",
    "        \n",
    "        # apply linear transformations\n",
    "        \n",
    "        # get packed Key, Query and Value matrices\n",
    "        # by multiplying inputs with K, Q and V weight matrices\n",
    "        K = torch.matmul(X, WK)\n",
    "        V = torch.matmul(X, WV)\n",
    "        Q = torch.matmul(inputs, WQ)\n",
    "            \n",
    "        # apply scaled dot-product attention\n",
    "        attention_scores = self.scaled_dot_product(Q, K, V)\n",
    "        \n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f2a08c",
   "metadata": {},
   "source": [
    ">An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. <sub>-from section 3.2 Attention<sub>\n",
    "\n",
    ">In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. <sub>-from section 3.2.1 Scaled Dot-Product Attention<sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91247af1",
   "metadata": {},
   "source": [
    "It can be inferred from the paper that K, Q and V weight matrices are distinct but in practice, in implementation, there is only **one weight matrix** and projected matrices are also one matrix. The reason for this is to implement the model with highly optimized matrix multiplication code.     \n",
    "We will not implement this way in this notebook because we want the matrices to be distinguishable so that the code could be easier to follow.\n",
    "\n",
    "<img src='assets/1/weight-impl.PNG' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297e759",
   "metadata": {},
   "source": [
    "In decoder attention layer, we get the linearly projected K and V matrices by considering encoder output.\n",
    "<img src='assets/1/encoder-out.PNG' width=50% height=50% align='left'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a4e87",
   "metadata": {},
   "source": [
    "Let's do a unit test on Attention implementation.\n",
    "\n",
    "<img src='assets/1/attention-matrix.PNG'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba8b0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from section 3.1 Encoder and Decoder Stacks\n",
    "d_model = 512    # embedding dimensionsionality\n",
    "\n",
    "b = 2   # batch size\n",
    "\n",
    "# define input\n",
    "# b, n, d_model -> batch size, sequence length(number of embeddings), embedding dimensionality\n",
    "X = torch.randn(b, n, d_model)\n",
    "\n",
    "# init attention \n",
    "attention = Attention(d_model, d_key)\n",
    "\n",
    "# compute attention scores\n",
    "attention_scores = attention(X)\n",
    "\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a877ac",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "<img src='assets/1/multi-head-attention-3.PNG' width=75% height=75%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6efb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention consists of several attention layers running in parallel.       \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head=8, d_model=512, mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.mask = mask\n",
    "        \n",
    "        # number of heads\n",
    "        self.h = n_head\n",
    "        \n",
    "        # dimensionality of key vector\n",
    "        assert d_model % n_head == 0\n",
    "        d_key = int(d_model / n_head)\n",
    "        \n",
    "        # add attention layers to a ModuleList container\n",
    "        attention_list = [Attention(d_model, d_key, mask) for _ in range(n_head)]\n",
    "        self.multi_head_attention = nn.ModuleList(attention_list)\n",
    "        \n",
    "        # define linear layer \n",
    "        self.W = nn.Parameter(torch.randn(n_head*d_key, d_model))\n",
    "    \n",
    "    def forward(self, x, encoder_output=None):\n",
    "        # apply & concat attention layers\n",
    "        attention_scores = [attention(x, encoder_output) for attention in self.multi_head_attention]\n",
    "        Z = torch.cat(attention_scores, -1)\n",
    "        \n",
    "        # apply linear transformation\n",
    "        output = torch.matmul(Z, self.W)    # reduce dimensionality\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# In this work we employ h = 8 parallel attention layers, or heads. \n",
    "# For each of these we use d_k = d_v = d_model/h = 64\n",
    "# - from section 3.2.2 Multi-Head Attention\n",
    "# We can conclude that d_key is not a hyperparameter\n",
    "# and that d_model should be divisible by the number of heads.        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e656d",
   "metadata": {},
   "source": [
    "Let's do a unit test on Multi-Head Attention implementation.\n",
    "\n",
    "<img src='assets/1/multi-head-matrix.PNG'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426095d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from section 3.2.2 Multi-Head Attention\n",
    "h = 8 # number of heads\n",
    "\n",
    "# define input\n",
    "X = torch.randn(b, n, d_model)\n",
    "\n",
    "# init multi-head attention \n",
    "multi_head_attention = MultiHeadAttention(h, d_model)\n",
    "\n",
    "# compute multi_head attention score\n",
    "Z = multi_head_attention(X)\n",
    "\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728135e",
   "metadata": {},
   "source": [
    "**Parallelization of Multi-Head Attention in Code**\n",
    "\n",
    "Multi-Head attention works with Attention layers running in parallel. However, currently there does  not exist an official implementation in PyTorch for parallel modules. Developers are using third-party libraries in their code to utilize parallelism. <br>\n",
    "Executing the code on GPU avoids sequential execution since underlying execution is asynchronous. Additionally, **if we have multiple GPUs in our system (and don’t use data parallel), we could execute different modules on each device and concatenate the result back on a single device.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a5fb1",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "<img src='assets/1/ffnn.PNG' width=75% height=75%/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb9e29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensionality of input and output is dmodel = 512, \n",
    "# and the inner-layer has dimensionality d_ff = 2048\n",
    "# - from section 3.3 Position-wise Feed-Forward Networks\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model=512, d_feedforward=2048):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        \n",
    "        # define the feedforward neural network \n",
    "        self.feedforwardnn = nn.Sequential(   \n",
    "                nn.Linear(d_model, d_feedforward),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_feedforward, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feedforwardnn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711a7c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test of FeedForward network\n",
    "\n",
    "# from section 3.3 Position-wise Feed-Forward Network\n",
    "d_feedforward = 2048\n",
    "\n",
    "# define input\n",
    "X = torch.rand(b, n, d_model)\n",
    "\n",
    "# define feedforward neural network\n",
    "ffnn = FeedForwardNetwork(d_model, d_feedforward)\n",
    "\n",
    "# compute ffnn output\n",
    "output = ffnn(X)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17775dc4",
   "metadata": {},
   "source": [
    "## Sublayer\n",
    "\n",
    "\n",
    "<img src='assets/1/sublayer.PNG' width=80% height=80%/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b73441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We employ a residual connection around each of the two sub-layers, \n",
    "# followed by layer normalization. That is'The output of each sub-layer is\n",
    "# LayerNorm(x + Sublayer(x)), where Sublayer(x) is\n",
    "# the function implemented by the sub-layer itself.'\n",
    "# from section - 3.1 Encoder and Decoder Stacks\n",
    "\n",
    "# define residual learning block\n",
    "class Residual_Connection(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super(Residual_Connection, self).__init__()\n",
    "        self.layer = layer     # function implemented by the layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        f_x = self.layer(x)                    # apply Sublayer(x)\n",
    "        x = x + f_x                            # x + Sublayer(x)\n",
    "        return x\n",
    "    \n",
    "# To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
    "# layers, produce outputs of dimension dmodel = 512\n",
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, layer, d_model=512):\n",
    "        super(Sublayer, self).__init__()\n",
    "    \n",
    "        self.res_connect = Residual_Connection(layer) \n",
    "        self.layer_norm = nn.LayerNorm(d_model)  # embedding vector length\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.res_connect(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "542b2ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 512])\n",
      "torch.Size([2, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# unit test on sublayer\n",
    "\n",
    "# define input\n",
    "X = torch.rand(b, n, d_model)\n",
    "\n",
    "# init feedforward neural network (with default settings) sublayer\n",
    "nn_layer = Sublayer(FeedForwardNetwork())\n",
    "\n",
    "# compute ffnn sublayer output\n",
    "ffnn_output = nn_layer(X)\n",
    "\n",
    "# init attention sublayer (with default settings)\n",
    "attention_layer = Sublayer(MultiHeadAttention())\n",
    "\n",
    "# compute attention sublayer output\n",
    "attention_output = attention_layer(X)\n",
    "\n",
    "print(ffnn_output.shape)\n",
    "print(attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8013c",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "<img src='assets/1/encoder.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ef60f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderwithSublayer(nn.Module):\n",
    "    def __init__(self, n_head=8, d_model=512, d_feedforward=2048):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(n_head, d_model)\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_feedforward)\n",
    "        \n",
    "        # two sub-layers\n",
    "        # 1. Self-Attention\n",
    "        self.attention_layer = Sublayer(self.attention, d_model)\n",
    "          \n",
    "        # 2. Feedforward Neural Network\n",
    "        self.feedforward_layer = Sublayer(self.feed_forward, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply attention layer\n",
    "        x = self.attention_layer(x)\n",
    "        # apply feedforward network layer\n",
    "        x = self.feedforward_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7d2289c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test on encoder\n",
    "\n",
    "# define input\n",
    "X = torch.rand(b, n, d_model)\n",
    "\n",
    "# init encoder\n",
    "encoder = EncoderWithSublayer(n_head=8, d_model=512, d_feedforward=2048)\n",
    "\n",
    "# compute encoder output\n",
    "output = encoder(X)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26148b77",
   "metadata": {},
   "source": [
    "We will not utilize Sublayer module implemented above to be able to observe the flow in Encoder & Decoder. Apart from that, we will not add any dropout to residual parts since we will not do any training in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0b48068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_head=8, d_model=512, d_feedforward=2048):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(n_head, d_model)\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_feedforward)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass through attention layer\n",
    "        attention_scores = self.attention(x)\n",
    "        # apply residual connection & layer normalization\n",
    "        x = self.layer_norm1(attention_scores + x)  \n",
    "        \n",
    "        # pass through feedforward network layer\n",
    "        scores = self.feed_forward(x)\n",
    "        # apply residual connection & layer normalization\n",
    "        x = self.layer_norm2(scores + x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecd943c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test on encoder\n",
    "\n",
    "# define input\n",
    "X = torch.rand(b, n, d_model)\n",
    "\n",
    "# init encoder\n",
    "encoder = Encoder(n_head=8, d_model=512, d_feedforward=2048)\n",
    "\n",
    "# compute encoder output\n",
    "output = encoder(X)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae56124",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "<img src='assets/1/decoder.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9aa64735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_head=8, d_model=512, d_feedforward=2048):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # three sub-layers\n",
    "        # 1. Masked Self-Attention\n",
    "        # 2. Self-Attention\n",
    "        # 3. Feedforward Neural Network\n",
    "        \n",
    "        self.masked_attention = MultiHeadAttention(n_head, d_model, mask=True)\n",
    "        self.attention = MultiHeadAttention(n_head, d_model)\n",
    "        self.feedforward = FeedForwardNetwork(d_model, d_feedforward)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output):\n",
    "        # apply masked attention layer\n",
    "        masked_attention_scores = self.masked_attention(x)\n",
    "        # apply residual connection & layer normalization\n",
    "        x = self.layer_norm1(masked_attention_scores + x)  \n",
    "        \n",
    "        # apply attention layer\n",
    "        attention_scores = self.attention(x, encoder_output)\n",
    "        x = self.layer_norm2(attention_scores + x)\n",
    "        \n",
    "        # apply feedforward network layer\n",
    "        scores = self.feedforward(x)\n",
    "        x = self.layer_norm3(scores + x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28258dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test on decoder\n",
    "\n",
    "# define input for decoder (remember that decoder input is the expected output)\n",
    "y = torch.rand(b, n, d_model)   \n",
    "encoder_output = torch.rand(b, n, d_model)\n",
    "\n",
    "# init decoder\n",
    "decoder = Decoder(n_head=8, d_model=512, d_feedforward=2048)\n",
    "\n",
    "# compute decoder output\n",
    "output = decoder(y, encoder_output)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada4838",
   "metadata": {},
   "source": [
    "## Positional Encoding \n",
    "\n",
    "(WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2cdba",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "<img src='assets/1/transformer-model.PNG' width=80% height=80%/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44678909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoders are all identical in structure (yet they do not share weights)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    # n is the number of stacked layers of encoders & decoders\n",
    "    def __init__(self, N=6, n_head=8, d_model=512, d_ff=2048):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # initialize the encoder and decoder stacks with sequential container\n",
    "        encoders, decoders = [], []\n",
    "        \n",
    "        # stack encoders\n",
    "        for i in range(N):\n",
    "            encoder = Encoder(n_head, d_model, d_ff)\n",
    "            encoders.append(encoder)\n",
    "        \n",
    "        # stack decoders\n",
    "        for i in range(N):\n",
    "            decoder = Decoder(n_head, d_model, d_ff)\n",
    "            decoders.append(decoder)\n",
    "            \n",
    "        self.encoders = nn.Sequential(*encoders)\n",
    "        self.decoders = nn.ModuleList(decoders) \n",
    "        \n",
    "        # define linear layer\n",
    "        self.W = nn.Parameter(torch.randn(n_head*d_key, d_model))\n",
    "        \n",
    "        # define softmax layer\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # embed and get positional\n",
    "        \n",
    "        # pass through encoders and get the output\n",
    "        encoder_output = self.encoders(x)\n",
    "        \n",
    "        # pass through decoders\n",
    "        decoder_output = y\n",
    "        for decoder in self.decoders:\n",
    "            decoder_output = decoder(decoder_output, encoder_output)\n",
    "        \n",
    "        # pass through linear layer\n",
    "        scores = torch.matmul(decoder_output, self.W)\n",
    "        \n",
    "        # pass through softmax\n",
    "        output = self.softmax(scores)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f24793d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test for transformer model\n",
    "\n",
    "# -> we define decoders as ModuleList \n",
    "# since Sequential container does not accept multiple inputs for forward method\n",
    "\n",
    "# from section 3.1 Encoder and Decoder Stacks\n",
    "N = 6\n",
    "\n",
    "# define input\n",
    "X = torch.rand(b, n, d_model)\n",
    "\n",
    "# define output\n",
    "y = torch.rand(b, n, d_model)\n",
    "\n",
    "# init transformer\n",
    "transformer = Transformer(N=6, n_head=8, d_model=512, d_ff=2048)\n",
    "\n",
    "# compute output\n",
    "output = transformer(X, y)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a721f",
   "metadata": {},
   "source": [
    "##  Comparing with PyTorch Transformer Implementation\n",
    "\n",
    "with nn.Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e245cc2",
   "metadata": {},
   "source": [
    "**Transformers Success**\n",
    "\n",
    "**Transformers Weakness**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a490bc",
   "metadata": {},
   "source": [
    "### Quick Recap of Familiar Components\n",
    "\n",
    "<img src='assets/1/components.PNG'/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09342dd9",
   "metadata": {},
   "source": [
    "### Final Notes\n",
    "\n",
    "The implementation here is a very basic representation of how Transformers work. There are a lot of lacking components that are avoided here to preserve the simplicity of the code. There are also a lot of improvements that can be made in the implementation here, and calculations can be further optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537e6bb",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "1. [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Blog Post\n",
    "2. [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) - Paper\n",
    "3. [Transformers for Beginners | What are they and how do they work](https://www.youtube.com/watch?v=_UVfwBqcnbM&t=4s) - Video\n",
    "4. [Implementing a Transformer from Scratch](https://towardsdatascience.com/7-things-you-didnt-know-about-the-transformer-a70d93ced6b2) - Blog Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc30b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
