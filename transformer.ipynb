{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b336f0c8",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0f63d",
   "metadata": {},
   "source": [
    "In this notebook, we are going to define and implement a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80b877da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a55cc6",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "<img src='assets/1/scaled-dot-product-3.PNG'/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eeae70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention calculation \n",
    "# from the paper (section 3.2.1 Scaled Dot-Product Attention):\n",
    "# 'We compute the dot products of the query with all keys, divide each by √dk, \n",
    "# and apply a softmax function to obtain the weights on the values.'\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=False):\n",
    "        \n",
    "        # compute dot product of the all query with all keys\n",
    "        dot_products = torch.matmul(Q, torch.transpose(K, 0, 1)) \n",
    "        \n",
    "        # divide each by √dk\n",
    "        d_k = K.shape[1]    # get length of key vector\n",
    "        scaled_dot_products = dot_products / np.sqrt(d_k)\n",
    "        \n",
    "        # apply a softmax function to obtain weights on values\n",
    "        weights = F.softmax(scaled_dot_products, dim=-1)\n",
    "        \n",
    "        # get weighted values by multiplying values with weights(softmax scores)\n",
    "        weighted_values = torch.matmul(weights, V)\n",
    "        \n",
    "        # apply mask to prevent positions from attending to subsequent positions in decoder\n",
    "        if mask==True:\n",
    "            pass\n",
    "            # look_ahead_mask = np.where(mask_ind, 1, -np.inf)\n",
    "            # weighted_values = weighted_values * look_ahead_mask  \n",
    "        \n",
    "        return weighted_values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf76d2",
   "metadata": {},
   "source": [
    "Scaled Dot-product attention is identical to the [Dot-product attention algorithm](https://arxiv.org/pdf/1508.04025.pdf), except for the scaling  factor of 1/√dk.\n",
    ">For large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by 1/√dk. <sub>-from section 3.2.1 Scaled Dot-Product Attention<sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb23674",
   "metadata": {},
   "source": [
    "**Masked Multi-Head Attention**\n",
    "\n",
    "<img src='assets/1/masked-attention.PNG' width=50% height=50% /> \n",
    "\n",
    ">We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We **implement this inside of scaled dot-product attention** by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. <sub>-from section 3.2.3 Applications of Attention in our Model<sub>\n",
    "\n",
    ">We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. <sub>-from section 3.1 Encoder and Decoder Stacks<sub>\n",
    "    \n",
    "Auto-regressive property can be defined as to predict future values based on past value. Therefore we need to mask the inputs that are subsequent to that position. So that, with having data given in parallel, the decoder will not learn a simple mapping provided all target outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e318e1",
   "metadata": {},
   "source": [
    "Let's do a unit test on Scaled Dot-Product Attention implementation.\n",
    "\n",
    "<img src='assets/1/scaled-dot-product-matrix.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91b4c5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define mock data assuming n=3 embeddings\n",
    "n = 3\n",
    "d_k = 64\n",
    "\n",
    "# define K,Q and V matrices\n",
    "K = torch.randn(n*d_k).reshape(n, d_k)\n",
    "Q = torch.randn(n*d_k).reshape(n, d_k)\n",
    "V = torch.randn(n*d_k).reshape(n, d_k)\n",
    "\n",
    "scaled_dot_product = ScaledDotProductAttention()\n",
    "output = scaled_dot_product(Q, K, V)    # apply scaled-dot product attention\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2541108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_output = scaled_dot_product(Q, K, V, mask=True)\n",
    "# masked_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6dca82",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "<img src='assets/1/attention-3.PNG'/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01083da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=512, d_k=64):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # define Key, Query and Value weight matrices\n",
    "        self.WK = nn.Parameter(torch.randn(d_model, d_k))\n",
    "        self.WQ = nn.Parameter(torch.randn(d_model, d_k))\n",
    "        self.WV = nn.Parameter(torch.randn(d_model, d_k))\n",
    "        \n",
    "        self.scaled_dot_product = ScaledDotProductAttention()\n",
    "        \n",
    "    def forward(self, inputs, mask=False):\n",
    "        WK, WQ, WV = self.WK, self.WQ, self.WV\n",
    "        \n",
    "        # apply linear transformations\n",
    "        \n",
    "        # get packed Key, Query and Value matrices\n",
    "        # by multiplying inputs with K, Q and V   \n",
    "        K = torch.matmul(inputs, WK)\n",
    "        Q = torch.matmul(inputs, WQ)\n",
    "        V = torch.matmul(inputs, WV)\n",
    "        \n",
    "        \n",
    "        # apply scaled dot-product attention\n",
    "        attention_scores = self.scaled_dot_product(Q, K, V, mask)\n",
    "        \n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05617d08",
   "metadata": {},
   "source": [
    ">An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. <sub>-from section 3.2 Attention<sub>\n",
    "\n",
    ">In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
    "into a matrix Q. The keys and values are also packed together into matrices K and V. <sub>-from section 3.2.1 Scaled Dot-Product Attention<sub>\n",
    "    \n",
    "To find a mapping between an input and output with respect to key-value pairs and a query, we define learnable parameters: key, query and value **weight matrices**. Linear transformation on input happens by multiplying input with Q, K and V weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a4e87",
   "metadata": {},
   "source": [
    "Let's do a unit test on Attention implementation.\n",
    "\n",
    "<img src='assets/1/attention-matrix.PNG'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ba8b0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 3   # input embeddings\n",
    "d_model = 512  # embedding dimensions\n",
    "d_k = 64  # key dimension\n",
    "\n",
    "# define input\n",
    "X = torch.randn(n*d_model).reshape(n, d_model)\n",
    "\n",
    "# init attention \n",
    "attention = Attention(d_model, d_k)\n",
    "\n",
    "# compute attention scores\n",
    "attention_scores = attention(X)\n",
    "\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a877ac",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "<img src='assets/1/multi-head-attention-3.PNG' width=75% height=75%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6efb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention consists of several attention layers running in parallel.       \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, h=8, d_model=512, d_k=64):\n",
    "        # add multi-head attention layers to a sequential container\n",
    "        multi_head_attention = []\n",
    "        \n",
    "        # define linear layer \n",
    "        self.W = nn.Parameter(torch.randn(h*d_k, d_model))\n",
    "    \n",
    "    def forward(self):\n",
    "        \n",
    "        # apply attention\n",
    "        \n",
    "        # collect attentions\n",
    "        \n",
    "        # concat attentions\n",
    "        attentions = torch.cat((x, x, x), 1)\n",
    "        \n",
    "        # apply linear transformation\n",
    "        multi_attentions = np.matmul(z, W)\n",
    "        # reduce dimensionality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd1e656d",
   "metadata": {},
   "source": [
    "Let's do a unit test on Multi-Head Attention implementation.\n",
    "\n",
    "<img src='assets/1/multi-head-matrix.PNG'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426095d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d27a5fb1",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "<img src='assets/1/ffnn.PNG' width=75% height=75%/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # feedforward neural network here\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_ff=2048):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear()\n",
    "        self.fc2 = nn.Linear()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a7c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's not skip the unit test of FeedForward network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17775dc4",
   "metadata": {},
   "source": [
    "## Sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b73441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We employ a residual connection around each of the two sub-layers, \n",
    "# followed by layer normalization. That is'The output of each sub-layer is\n",
    "# LayerNorm(x + Sublayer(x)), where Sublayer(x) is\n",
    "# the function implemented by the sub-layer itself.'\n",
    "# from section - 3.1 Encoder and Decoder Stacks\n",
    "\n",
    "# define residual learning block\n",
    "class Residual_Connection(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super(Residual_Connection, self).__init__()\n",
    "        self.layer = layer     # function implemented by the sublayer\n",
    "\n",
    "    def forward(self, x):\n",
    "        f_x = self.layer(x)    # apply Sublayer(x)\n",
    "        x = x + f_x            # x + Sublayer(x)\n",
    "        return x\n",
    "    \n",
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super(SubLayer, self).__init__()\n",
    "    \n",
    "        self.sublayer = nn.Sequential(\n",
    "                            Residual_Connection(layer), \n",
    "                            nn.LayerNorm()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sublayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test on sublayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8013c",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "<img src='assets/1/encoder.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef60f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention()\n",
    "        self.feed_forward = \n",
    "        \n",
    "        # two sub-layers\n",
    "        # 1. Self-Attention\n",
    "        self.sublayer1 = Sublayer(self.attention)\n",
    "          \n",
    "        # 2. Feedforward Neural Network\n",
    "        self.sublayer2 = Sublayer(self.attention)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # nn.Sequential(*[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae56124",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "<img src='assets/1/decoder.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa64735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # three sub-layers\n",
    "        # 1. Feedforward Neural Network\n",
    "        # 2. Self-Attention\n",
    "        # 3. Self-Attention\n",
    "        \n",
    "        # employ a residual connection around each of\n",
    "        # the two sub-layers, followed by layer normalization\n",
    "        \n",
    "        # That is, the output of each sub-layer is\n",
    "        # LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function \n",
    "        # implemented by the sub-layer itself\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2cdba",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "<img src='assets/1/transformer-model.PNG' width=80% height=80%/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44678909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoders are all identical in structure (yet they do not share weights)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    # N is the number of stacked layers of encoders & decoders\n",
    "    def __init__(self, N=6, h=8, d_k=64, d_v=64):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # initialize the encoder and decoder stacks with sequential container\n",
    "        encoders, decoders = [], []\n",
    "        \n",
    "        # stack encoders\n",
    "        for i in range(N):\n",
    "            encoder = Encoder()\n",
    "            encoders.append(encoder)\n",
    "        \n",
    "        # stack decoders\n",
    "        for i in range(N):\n",
    "            decoder = Decoder()\n",
    "            decoders.append(decoder)\n",
    "            \n",
    "        encoders = nn.Sequential(*encoders)\n",
    "        decoders = nn.Sequential(*decoders)\n",
    "        \n",
    "    def forward(self):\n",
    "        # pass through encoders\n",
    "        \n",
    "        # pass through decoders but considering same input\n",
    "        pass\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a721f",
   "metadata": {},
   "source": [
    "##  Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc757b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "\n",
    "# from section 3.1 Encoder and Decoder Stacks\n",
    "N = 6  # Number of layers in the encoder stack and decoder stack\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "\n",
    "# from section 3.2.2 Multi-Head Attention\n",
    "# d_k = d_v = d_model / h = 64\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "\n",
    "# from section 3.3 Position-wise Feed-Forward Network\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f797a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training parameters\n",
    "\n",
    "# from section 5.3 Optimizer \n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "warmup_steps = 4000\n",
    "# train steps is stated as 100K in the paper, \n",
    "# we will use warmup steps to train \n",
    "\n",
    "# from section 5.4 Regularization\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fdd381",
   "metadata": {},
   "source": [
    "## Transformers Success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647a5b8",
   "metadata": {},
   "source": [
    "### Final Notes\n",
    "\n",
    "The implementation here is a very basic representation of how Transformers work. In practice, the Transformer model is used with batches with sequence lengths. This introduces new dimensions in the tensors discussed above, and also a lot of new calculations, complex matrix multiplications. There are a lot of improvements that can be made in the implementation here, and a lot of calculations can be further optimized.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537e6bb",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "1. [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Blog Post\n",
    "2. [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) - Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc30b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
