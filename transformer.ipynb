{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b336f0c8",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0f63d",
   "metadata": {},
   "source": [
    "In this notebook, we are going to define and implement a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b877da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a55cc6",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "<img src='assets/1/scaled-dot-product-3.PNG'/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeae70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention calculation \n",
    "# from the paper (section 3.2.1 Scaled Dot-Product Attention):\n",
    "# 'We compute the dot products of the query with all keys, divide each by √dk, \n",
    "# and apply a softmax function to obtain the weights on the values.'\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, mask=False):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.mask=mask\n",
    "        \n",
    "    def forward(self, Q, K, V):\n",
    "        \n",
    "        # compute dot product of the all query with all keys\n",
    "        K_transpose = torch.transpose(K, -2, -1)   # transpose on last two dimensions\n",
    "        dot_products = torch.matmul(Q, K_transpose) \n",
    "        \n",
    "        # divide each by √dk\n",
    "        d_k = K.shape[-1]    # get length of key vector\n",
    "        scaled_dot_products = dot_products / np.sqrt(d_k)\n",
    "        \n",
    "        # apply a softmax function to obtain weights(attention scores) on values\n",
    "        weights = F.softmax(scaled_dot_products, dim=-1)\n",
    "        \n",
    "        # get weighted values by multiplying values with softmax scores\n",
    "        weighted_values = torch.matmul(weights, V)\n",
    "        \n",
    "        # apply mask to prevent positions from attending to subsequent positions in decoder\n",
    "        if self.mask==True:\n",
    "            size = weighted_values.shape\n",
    "            look_ahead_mask = torch.triu(torch.full(size, float('-inf')), diagonal=1)\n",
    "            look_ahead_mask[look_ahead_mask == 0] = 1\n",
    "            weighted_values = weighted_values * look_ahead_mask\n",
    "        \n",
    "        return weighted_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf76d2",
   "metadata": {},
   "source": [
    "Scaled Dot-product attention is identical to the [Dot-product attention algorithm](https://arxiv.org/pdf/1508.04025.pdf), except for the scaling  factor of 1/√dk.\n",
    ">For large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by 1/√dk. <br><sub>-from section 3.2.1 Scaled Dot-Product Attention<sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb23674",
   "metadata": {},
   "source": [
    "**Masked Multi-Head Attention**\n",
    "\n",
    "<img src='assets/1/masked-attention.PNG' width=50% height=50% /> \n",
    "\n",
    ">We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We **implement this inside of scaled dot-product attention** by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. <br><sub>-from section 3.2.3 Applications of Attention in our Model<sub>\n",
    "\n",
    ">We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. <br><sub>-from section 3.1 Encoder and Decoder Stacks<sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e318e1",
   "metadata": {},
   "source": [
    "Let's do a unit test on Scaled Dot-Product Attention implementation.\n",
    "\n",
    "<img src='assets/1/scaled-dot-product-matrix.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b4c5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define data, n=3 embeddings(sequence length or number of tokens), ex: [Je, suis, etudiant]\n",
    "n = 3\n",
    "\n",
    "# from section 3.2.2 Multi-Head Attention\n",
    "# d_key = d_value = d_query\n",
    "d_key = 64 # key dimensionality\n",
    "\n",
    "# define K,Q and V matrices\n",
    "K = torch.randn(n, d_key)\n",
    "Q = torch.randn(n, d_key)\n",
    "V = torch.randn(n, d_key)\n",
    "\n",
    "scaled_dot_product = ScaledDotProductAttention()\n",
    "\n",
    "# apply scaled-dot product attention\n",
    "attention_scores = scaled_dot_product(Q, K, V)    \n",
    "\n",
    "assert attention_scores.shape == (n, d_key)\n",
    "attention_scores.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2541108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64])\n",
      "tensor([[-0.2168,    -inf,     inf,    -inf,     inf],\n",
      "        [ 0.4870,  1.5015,    -inf,    -inf,    -inf],\n",
      "        [-0.1390,  1.0056, -1.1646,    -inf,     inf]])\n"
     ]
    }
   ],
   "source": [
    "# unit test for masked scaled dot-product attention \n",
    "\n",
    "scaled_dot_product = ScaledDotProductAttention(mask=True)\n",
    "masked_attn_scores = scaled_dot_product(Q, K, V)\n",
    "\n",
    "assert masked_attn_scores.shape == (n, d_key)\n",
    "print(masked_attn_scores.shape)\n",
    "\n",
    "print(masked_attn_scores[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6dca82",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "<img src='assets/1/attention-3.PNG'/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01083da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=512, d_key=64, mask=False):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # define Key, Query and Value weight matrices\n",
    "        self.WK = nn.Parameter(torch.randn(d_model, d_key))\n",
    "        self.WQ = nn.Parameter(torch.randn(d_model, d_key))\n",
    "        self.WV = nn.Parameter(torch.randn(d_model, d_key))\n",
    "        \n",
    "        # init scaled dot-product attention\n",
    "        self.scaled_dot_product = ScaledDotProductAttention(mask)\n",
    "        \n",
    "    def forward(self, inputs, encoder_output=None):\n",
    "        WK, WQ, WV = self.WK, self.WQ, self.WV\n",
    "        X = inputs if encoder_output is None else encoder_output\n",
    "        \n",
    "        # apply linear transformations\n",
    "        \n",
    "        # get packed Key, Query and Value matrices\n",
    "        # by multiplying inputs with K, Q and V weight matrices\n",
    "        K = torch.matmul(X, WK)\n",
    "        V = torch.matmul(X, WV)\n",
    "        Q = torch.matmul(inputs, WQ)\n",
    "            \n",
    "        # apply scaled dot-product attention\n",
    "        attention_scores = self.scaled_dot_product(Q, K, V)\n",
    "        \n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f2a08c",
   "metadata": {},
   "source": [
    ">An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. <br><sub>-from section 3.2 Attention<sub>\n",
    "\n",
    ">In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. <br><sub>-from section 3.2.1 Scaled Dot-Product Attention<sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91247af1",
   "metadata": {},
   "source": [
    "It can be inferred from the paper that K, Q and V weight matrices are distinct but in practice, in implementation, there is only **one weight matrix** and projected matrices are also one matrix. The reason for this is to implement the model with highly optimized matrix multiplication code.     \n",
    "We will not implement this way in this notebook because we want the matrices to be distinguishable so that the code could be easier to follow.\n",
    "\n",
    "<img src='assets/1/weight-impl.PNG' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297e759",
   "metadata": {},
   "source": [
    "In decoder attention layer, we get the linearly projected K and V matrices by considering encoder output.\n",
    "<img src='assets/1/encoder-out.PNG' width=50% height=50% align='left'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a4e87",
   "metadata": {},
   "source": [
    "Let's do a unit test on Attention implementation.\n",
    "\n",
    "<img src='assets/1/attention-matrix.PNG'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba8b0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from section 3.1 Encoder and Decoder Stacks\n",
    "d_model = 512    # embedding dimensionsionality\n",
    "\n",
    "b = 2   # batch size\n",
    "\n",
    "# define input\n",
    "# b, n, d_model -> batch size, sequence length(number of embeddings), embedding dimensionality\n",
    "X = torch.randn(b, n, d_model)\n",
    "\n",
    "# init attention \n",
    "attention = Attention(d_model, d_key)\n",
    "\n",
    "# compute attention scores\n",
    "attention_scores = attention(X)\n",
    "\n",
    "assert attention_scores.shape == (b, n, d_key)\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a877ac",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "<img src='assets/1/multi-head-attention-3.PNG' width=75% height=75%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6efb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention consists of several attention layers running in parallel.       \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head=8, d_model=512, mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.mask = mask\n",
    "        \n",
    "        # number of heads\n",
    "        self.h = n_head\n",
    "        \n",
    "        # dimensionality of key vector\n",
    "        assert d_model % n_head == 0\n",
    "        d_key = int(d_model / n_head)\n",
    "        \n",
    "        # add attention layers to a ModuleList container\n",
    "        attention_list = [Attention(d_model, d_key, mask) for _ in range(n_head)]\n",
    "        self.multi_head_attention = nn.ModuleList(attention_list)\n",
    "        \n",
    "        # define linear layer \n",
    "        self.W = nn.Parameter(torch.randn(n_head*d_key, d_model))\n",
    "    \n",
    "    def forward(self, x, encoder_output=None):\n",
    "        # apply & concat attention layers\n",
    "        attention_scores = [attention(x, encoder_output) for attention in self.multi_head_attention]\n",
    "        Z = torch.cat(attention_scores, -1)\n",
    "        \n",
    "        # apply linear transformation\n",
    "        output = torch.matmul(Z, self.W)    # reduce dimensionality\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# In this work we employ h = 8 parallel attention layers, or heads. \n",
    "# For each of these we use d_k = d_v = d_model/h = 64\n",
    "# - from section 3.2.2 Multi-Head Attention\n",
    "# We can conclude that d_key is not a hyperparameter\n",
    "# and that d_model should be divisible by the number of heads.        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e656d",
   "metadata": {},
   "source": [
    "Let's do a unit test on Multi-Head Attention implementation.\n",
    "\n",
    "<img src='assets/1/multi-head-matrix.PNG'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "426095d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from section 3.2.2 Multi-Head Attention\n",
    "h = 8 # number of heads\n",
    "\n",
    "# define input\n",
    "X = torch.randn(b, n, d_model)\n",
    "\n",
    "# init multi-head attention \n",
    "multi_head_attention = MultiHeadAttention(h, d_model)\n",
    "\n",
    "# compute multi_head attention score\n",
    "Z = multi_head_attention(X)\n",
    "\n",
    "assert Z.shape == (b, n, d_model)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a5fb1",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "<img src='assets/1/ffnn.PNG' width=75% height=75%/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb9e29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensionality of input and output is dmodel = 512, \n",
    "# and the inner-layer has dimensionality d_ff = 2048\n",
    "# - from section 3.3 Position-wise Feed-Forward Networks\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model=512, d_feedforward=2048):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        \n",
    "        # define the feedforward neural network \n",
    "        self.feedforwardnn = nn.Sequential(   \n",
    "                nn.Linear(d_model, d_feedforward),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_feedforward, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feedforwardnn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711a7c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test of FeedForward network\n",
    "\n",
    "# from section 3.3 Position-wise Feed-Forward Network\n",
    "d_feedforward = 2048\n",
    "\n",
    "# define input\n",
    "X = torch.rand(b, n, d_model)\n",
    "\n",
    "# define feedforward neural network\n",
    "ffnn = FeedForwardNetwork(d_model, d_feedforward)\n",
    "\n",
    "# compute ffnn output\n",
    "output = ffnn(X)\n",
    "\n",
    "assert output.shape == (b, n, d_model)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17775dc4",
   "metadata": {},
   "source": [
    "## Sublayer\n",
    "\n",
    "\n",
    "<img src='assets/1/sublayer.PNG' width=80% height=80%/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b73441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We employ a residual connection around each of the two sub-layers, \n",
    "# followed by layer normalization. That is'The output of each sub-layer is\n",
    "# LayerNorm(x + Sublayer(x)), where Sublayer(x) is\n",
    "# the function implemented by the sub-layer itself.'\n",
    "# from section - 3.1 Encoder and Decoder Stacks\n",
    "\n",
    "# define residual learning block\n",
    "class Residual_Connection(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super(Residual_Connection, self).__init__()\n",
    "        self.layer = layer     # function implemented by the layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        f_x = self.layer(x)                    # apply Sublayer(x)\n",
    "        x = x + f_x                            # x + Sublayer(x)\n",
    "        return x\n",
    "    \n",
    "# To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
    "# layers, produce outputs of dimension dmodel = 512\n",
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, layer, d_model=512):\n",
    "        super(Sublayer, self).__init__()\n",
    "    \n",
    "        self.res_connect = Residual_Connection(layer) \n",
    "        self.layer_norm = nn.LayerNorm(d_model)  # embedding vector length\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.res_connect(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "542b2ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 512])\n",
      "torch.Size([2, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# unit test on sublayer\n",
    "\n",
    "# define input\n",
    "X = torch.rand(b, n, d_model)\n",
    "\n",
    "# init feedforward neural network (with default settings) sublayer\n",
    "nn_layer = Sublayer(FeedForwardNetwork())\n",
    "\n",
    "# compute ffnn sublayer output\n",
    "ffnn_output = nn_layer(X)\n",
    "\n",
    "# init attention sublayer (with default settings)\n",
    "attention_layer = Sublayer(MultiHeadAttention())\n",
    "\n",
    "# compute attention sublayer output\n",
    "attention_output = attention_layer(X)\n",
    "\n",
    "assert ffnn_output.shape == (b, n, d_model)\n",
    "assert attention_output.shape == (b, n, d_model)\n",
    "\n",
    "print(ffnn_output.shape)\n",
    "print(attention_output.shape)\n",
    "\n",
    "# We will not utilize Sublayer module implemented above to observe the flow in Encoder & Decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8013c",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "<img src='assets/1/encoder.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a7bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_head=8, d_model=512, d_feedforward=2048):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(n_head, d_model)\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_feedforward)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass through attention layer\n",
    "        attention_scores = self.attention(x)\n",
    "        # apply residual connection & layer normalization\n",
    "        x = self.layer_norm1(attention_scores + x)  \n",
    "        \n",
    "        # pass through feedforward network layer\n",
    "        scores = self.feed_forward(x)\n",
    "        # apply residual connection & layer normalization\n",
    "        x = self.layer_norm2(scores + x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# We will not add any dropout since we will not do any training in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02edcef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test on encoder\n",
    "\n",
    "# The encoders are all identical in structure (yet they do not share weights)\n",
    "\n",
    "# define input\n",
    "X = torch.rand(b, n, d_model)\n",
    "\n",
    "# init encoder\n",
    "encoder = Encoder(n_head=8, d_model=512, d_feedforward=2048)\n",
    "\n",
    "# compute encoder output\n",
    "output = encoder(X)\n",
    "\n",
    "assert output.shape == (b, n, d_model)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae56124",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "<img src='assets/1/decoder.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aa64735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_head=8, d_model=512, d_feedforward=2048):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # three sub-layers\n",
    "        # 1. Masked Self-Attention\n",
    "        # 2. Self-Attention\n",
    "        # 3. Feedforward Neural Network\n",
    "        \n",
    "        self.masked_attention = MultiHeadAttention(n_head, d_model, mask=True)\n",
    "        self.attention = MultiHeadAttention(n_head, d_model)\n",
    "        self.feedforward = FeedForwardNetwork(d_model, d_feedforward)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output):\n",
    "        # apply masked attention layer\n",
    "        masked_attention_scores = self.masked_attention(x)\n",
    "        # apply residual connection & layer normalization\n",
    "        x = self.layer_norm1(masked_attention_scores + x)  \n",
    "        \n",
    "        # apply attention layer\n",
    "        attention_scores = self.attention(x, encoder_output)\n",
    "        x = self.layer_norm2(attention_scores + x)\n",
    "        \n",
    "        # apply feedforward network layer\n",
    "        scores = self.feedforward(x)\n",
    "        x = self.layer_norm3(scores + x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28258dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test on decoder\n",
    "\n",
    "# define input for decoder (remember that decoder input is the expected output)\n",
    "y = torch.rand(b, n, d_model)   \n",
    "encoder_output = torch.rand(b, n, d_model)\n",
    "\n",
    "# init decoder\n",
    "decoder = Decoder(n_head=8, d_model=512, d_feedforward=2048)\n",
    "\n",
    "# compute decoder output\n",
    "output = decoder(y, encoder_output)\n",
    "\n",
    "assert output.shape == (b, n, d_model)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ada5d",
   "metadata": {},
   "source": [
    "## Positional Encoding \n",
    "\n",
    "<img src='assets/1/pos-encoding.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d25b937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # init positional encoding matrix\n",
    "        PE = torch.zeros(x.shape[1:])    # exclude batch size\n",
    "        \n",
    "        # init positions (sequence, number of tokens)\n",
    "        position = torch.arange(0, PE.shape[0])\n",
    "        # init i(dimensions)\n",
    "        i = torch.arange(0, self.d_model // 2)  # since 2i is -> d, we get i -> d / 2\n",
    "        \n",
    "        # division term -> 10000 ^ (2i / d_model) \n",
    "        div_term = torch.pow(10000, 2 * i / self.d_model) \n",
    "        \n",
    "        # apply sin to even indices(columns) in the position; 2i\n",
    "        PE[:, 0::2] = torch.sin(position[:, np.newaxis] / div_term)\n",
    "        \n",
    "        # apply cos to odd indices(columns) in the position; 2i + 1\n",
    "        PE[:, 1::2] = torch.cos(position[:, np.newaxis] / div_term)\n",
    "        \n",
    "        return PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf4780",
   "metadata": {},
   "source": [
    ">Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. <br><sub>-from section 3.5 Positional Encoding<sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894664e",
   "metadata": {},
   "source": [
    "<img src='assets/1/pos-encoding-2.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2d4c299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test for positional encoding \n",
    "\n",
    "# define input (assume input is embedded) \n",
    "X = torch.randn(b, n, d_model)\n",
    "\n",
    "# init positional encoding \n",
    "pos_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "# compute and add positional encoding of the embedding inputs \n",
    "embeddings = X + pos_encoding(X) \n",
    "\n",
    "assert embeddings.shape == (b, n, d_model)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89517d1a",
   "metadata": {},
   "source": [
    "## Embedding and Pre-Softmax Linear Layer \n",
    "\n",
    "<img src='assets/1/embed.PNG' width=45% height=45% align='left'/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3726819",
   "metadata": {},
   "source": [
    ">Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, **we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation**. In the embedding layers, we multiply those weights by √dmodel.<br><sub>-from section 3.4 Embeddings and Softmax<sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2cdba",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "<img src='assets/1/transformer-model.PNG' width=80% height=80%/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44678909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):    \n",
    "    def __init__(self, n_token=37000, n_layer=6, n_head=8, d_model=512, d_ff=2048):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.n_token = n_token\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "\n",
    "        # initialize the encoder and decoder stacks\n",
    "        encoders, decoders = [], []\n",
    "        \n",
    "        # stack encoders\n",
    "        for i in range(n_layer):\n",
    "            encoder = Encoder(n_head, d_model, d_ff)\n",
    "            encoders.append(encoder)\n",
    "        \n",
    "        # stack decoders\n",
    "        for i in range(n_layer):\n",
    "            decoder = Decoder(n_head, d_model, d_ff)\n",
    "            decoders.append(decoder)\n",
    "            \n",
    "        self.encoders = nn.Sequential(*encoders)\n",
    "        self.decoders = nn.ModuleList(decoders)\n",
    "        \n",
    "        # the encoder embedding, and decoder embedding \n",
    "        # and decoder pre-softmax transformation(linear layer)\n",
    "        # share embeddings weights\n",
    "        #\n",
    "        # define embedding layer\n",
    "        self.embedding = nn.Embedding(n_token, d_model)   # num_embeddings, embedding_dim\n",
    "        \n",
    "        # define positional encoding layer \n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        # define softmax layer\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # embed inputs & outputs and get positional encoding\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding(x)\n",
    "        \n",
    "        y = self.embedding(y) * np.sqrt(self.d_model)\n",
    "        y = y + self.pos_encoding(y)\n",
    "        \n",
    "        # pass through encoders and get the output\n",
    "        encoder_output = self.encoders(x)\n",
    "        \n",
    "        # pass through decoders\n",
    "        decoder_output = y\n",
    "        for decoder in self.decoders:\n",
    "            decoder_output = decoder(decoder_output, encoder_output)\n",
    "        \n",
    "        # pass through linear layer\n",
    "        # \n",
    "        # a linear layer multiplies the input with a transpose of the weight matrix\n",
    "        scores = torch.matmul(\n",
    "            decoder_output, torch.transpose(self.embedding.weight, -2, -1)\n",
    "        )\n",
    "        \n",
    "        # pass through softmax\n",
    "        output = self.softmax(scores)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92b391",
   "metadata": {},
   "source": [
    "<img src='assets/1/transformer-comp.PNG' width=40% height=40% align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f24793d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 37000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test for transformer model\n",
    "\n",
    "# -> we define decoders as ModuleList \n",
    "# since Sequential container does not accept multiple inputs for forward method\n",
    "\n",
    "# from section 3.1 Encoder and Decoder Stacks\n",
    "# number of stacked layers of encoders & decoders\n",
    "n_layer = 6\n",
    "\n",
    "# from section 5.1 Training Data and Batching\n",
    "n_token = 37000   # vocabulary size\n",
    "\n",
    "# define input, \n",
    "# since embedding expects token indices, init as LongTensor(torch.int64) \n",
    "X = torch.randint(0, n_token, (b, n))\n",
    "\n",
    "# define output \n",
    "y = torch.randint(0, n_token, (b, n))\n",
    "\n",
    "# shift to right\n",
    "# to ensure that predictions at a specific position \"i\" can only depend at positions less than i\n",
    "y = torch.roll(y, 1)\n",
    "\n",
    "# init transformer\n",
    "transformer = Transformer(n_token=37000, n_layer=6, n_head=8, d_model=512, d_ff=2048)\n",
    "\n",
    "# compute output\n",
    "output = transformer(X, y)\n",
    "\n",
    "assert output.shape == (b, n, n_token)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69923d5",
   "metadata": {},
   "source": [
    "---\n",
    "**Parallelization of Multi-Head Attention in Code**\n",
    "\n",
    "Multi-Head attention works with Attention layers running in parallel. However, currently there does  not exist an official implementation in PyTorch for parallel modules. Developers are using third-party libraries in their code to utilize parallelism. <br>\n",
    "Executing the code on GPU avoids sequential execution since underlying execution is asynchronous. **If we have multiple GPUs in our system (and don’t use data parallel), we could execute different modules on each device and concatenate the result back on a single device.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709ebdb",
   "metadata": {},
   "source": [
    "---\n",
    "**Key, Value and Query**\n",
    "\n",
    "The key/value/query concept is analogous to retrieval systems. For example, when we search for videos on Youtube, the search engine will map our query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present us the best matched videos (values). The dot product can be considered as defining some similarity between the text in search bar (query) and titles in the database (key)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e245cc2",
   "metadata": {},
   "source": [
    "---\n",
    "**Transformers Success**\n",
    "\n",
    ">We propose a new simple network architecture, the Transformer,\n",
    "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
    "entirely. Experiments on two machine translation tasks show these models to\n",
    "be superior in quality while being more parallelizable and requiring significantly\n",
    "less time to train. <br><sub>-from section Abstract<sub><br>\n",
    "\n",
    ">Motivating our use of self-attention we consider three desiderata.\n",
    "One is the total computational complexity per layer. Another is the amount of computation that can\n",
    "be parallelized, as measured by the minimum number of sequential operations required.\n",
    "The third is the path length between long-range dependencies in the network.<br><sub>-from section 4 Why Self-Attention<sub><br>\n",
    "    \n",
    "The major success of the Transformer model is that it is parallelizable. Formerly, with recurrent models, we were not able to parallelize since each time step outputs of sequences were depending on previous time step outputs. With Transformer, we can feed the data to the model at the same time and can obtain output. This leads to faster training compared to other RNN & LSTM variant sequence to sequence recurrent models. Transformer model is considered as the state-of-art for language models and NLP.\n",
    "\n",
    "**Transformers Weakness**<br>\n",
    "\n",
    "Transformer model requires a significant amount of data to train effectively. Similar to other deep learning models, it is prone to overfitting if trained with small amount of data. Another drawback is, the high computational cost required to run a Transformer effectively. The self-attention mechanism requires a lot of computation, making it more resource-intensive than other deep learning models.  \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a490bc",
   "metadata": {},
   "source": [
    "### Quick Recap of Familiar Components\n",
    "\n",
    "<img src='assets/1/components.PNG'/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09342dd9",
   "metadata": {},
   "source": [
    "### Final Notes\n",
    "\n",
    "The implementation here is a very basic representation of how Transformers work. There are a lot of lacking components that are avoided here to preserve the simplicity of the code. There are also a lot of improvements that can be made in the implementation here, and calculations can be further optimized. <br>\n",
    "\n",
    "The training part and the data handling part is not discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537e6bb",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "1. [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Blog Post\n",
    "2. [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) - Paper\n",
    "3. [Transformers for Beginners | What are they and how do they work](https://www.youtube.com/watch?v=_UVfwBqcnbM&t=4s) - Video\n",
    "4. [Implementing a Transformer from Scratch](https://towardsdatascience.com/7-things-you-didnt-know-about-the-transformer-a70d93ced6b2) - Blog Post\n",
    "5. [Transformer’s Encoder-Decoder: Let’s Understand The Model Architecture](https://kikaben.com/transformers-encoder-decoder/) - Blog Post\n",
    "6. [A Gentle Introduction to Positional Encoding in Transformer Models, Part 1](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/#:~:text=Positional%20encoding%20describes%20the%20location,item's%20position%20in%20transformer%20models.) - Blog Post\n",
    "7. [Visual Guide to Transformer Neural Networks - (Episode 1) Position Embeddings](https://www.youtube.com/watch?v=dichIcUZfOw) - Video\n",
    "8. [Transformer Model Inference Visualization](https://miro.medium.com/v2/resize:fit:1100/1*HDqn93FsA93o2z4bTwqUVQ.gif) - GIF\n",
    "9. [What exactly are keys, queries, and values in attention mechanisms?](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc30b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
