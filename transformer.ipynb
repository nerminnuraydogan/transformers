{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b336f0c8",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0f63d",
   "metadata": {},
   "source": [
    "In this notebook, we are going to define and implement a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b877da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a55cc6",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "<img src='assets/1/scaled-dot-product-3.PNG'/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeae70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention calculation \n",
    "# from the paper (section 3.2.1 Scaled Dot-Product Attention):\n",
    "# 'We compute the dot products of the query with all keys, divide each by √dk, \n",
    "# and apply a softmax function to obtain the weights on the values.'\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, mask=False):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.mask=mask\n",
    "        \n",
    "    def forward(self, Q, K, V):\n",
    "        \n",
    "        # compute dot product of the all query with all keys\n",
    "        dot_products = torch.matmul(Q, torch.transpose(K, 0, 1)) \n",
    "        \n",
    "        # divide each by √dk\n",
    "        d_k = K.shape[1]    # get length of key vector\n",
    "        scaled_dot_products = dot_products / np.sqrt(d_k)\n",
    "        \n",
    "        # apply a softmax function to obtain weights on values\n",
    "        weights = F.softmax(scaled_dot_products, dim=-1)\n",
    "        \n",
    "        # get weighted values by multiplying values with weights(softmax scores)\n",
    "        weighted_values = torch.matmul(weights, V)\n",
    "        \n",
    "        # apply mask to prevent positions from attending to subsequent positions in decoder\n",
    "        if self.mask==True:\n",
    "            size = weighted_values.shape \n",
    "            look_ahead_mask = torch.triu(torch.full(size, float('-inf')), diagonal=1)\n",
    "            look_ahead_mask[look_ahead_mask == 0] = 1  \n",
    "            weighted_values = weighted_values * look_ahead_mask\n",
    "        \n",
    "        return weighted_values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf76d2",
   "metadata": {},
   "source": [
    "Scaled Dot-product attention is identical to the [Dot-product attention algorithm](https://arxiv.org/pdf/1508.04025.pdf), except for the scaling  factor of 1/√dk.\n",
    ">For large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by 1/√dk. <sub>-from section 3.2.1 Scaled Dot-Product Attention<sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb23674",
   "metadata": {},
   "source": [
    "**Masked Multi-Head Attention**\n",
    "\n",
    "<img src='assets/1/masked-attention.PNG' width=50% height=50% /> \n",
    "\n",
    ">We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We **implement this inside of scaled dot-product attention** by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. <sub>-from section 3.2.3 Applications of Attention in our Model<sub>\n",
    "\n",
    ">We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. <sub>-from section 3.1 Encoder and Decoder Stacks<sub>\n",
    "    \n",
    "Auto-regressive property can be defined as to predict future values based on past value. Therefore we need to mask the inputs that are subsequent to that position. So that, with having data given in parallel, the decoder will not learn a simple mapping provided all target outputs.\n",
    "    \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e318e1",
   "metadata": {},
   "source": [
    "Let's do a unit test on Scaled Dot-Product Attention implementation.\n",
    "\n",
    "<img src='assets/1/scaled-dot-product-matrix.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b4c5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define data, n=3 embeddings\n",
    "n = 3\n",
    "\n",
    "# from section 3.2.2 Multi-Head Attention\n",
    "d_key = 64 # key dimension\n",
    "\n",
    "# define K,Q and V matrices\n",
    "K = torch.randn(n, d_key)\n",
    "Q = torch.randn(n, d_key)\n",
    "V = torch.randn(n, d_key)\n",
    "\n",
    "scaled_dot_product = ScaledDotProductAttention()\n",
    "\n",
    "# apply scaled-dot product attention\n",
    "output = scaled_dot_product(Q, K, V)    \n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2541108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64])\n",
      "tensor([[-0.6561,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.6787,  0.1006,    -inf,    -inf,    -inf],\n",
      "        [-0.6778, -0.2175, -0.0867,     inf,     inf]])\n"
     ]
    }
   ],
   "source": [
    "scaled_dot_product = ScaledDotProductAttention(mask=True)\n",
    "\n",
    "masked_output = scaled_dot_product(Q, K, V)\n",
    "print(masked_output.shape)\n",
    "\n",
    "print(masked_output[:,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6dca82",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "<img src='assets/1/attention-3.PNG'/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01083da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=512, d_key=64, mask=False):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # define Key, Query and Value weight matrices\n",
    "        self.WK = nn.Parameter(torch.randn(d_model, d_key))\n",
    "        self.WQ = nn.Parameter(torch.randn(d_model, d_key))\n",
    "        self.WV = nn.Parameter(torch.randn(d_model, d_key))\n",
    "        \n",
    "        # init scaled dot-product attention\n",
    "        self.scaled_dot_product = ScaledDotProductAttention(mask)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        WK, WQ, WV = self.WK, self.WQ, self.WV\n",
    "        \n",
    "        # apply linear transformations\n",
    "        \n",
    "        # get packed Key, Query and Value matrices\n",
    "        # by multiplying inputs with K, Q and V   \n",
    "        K = torch.matmul(inputs, WK)\n",
    "        Q = torch.matmul(inputs, WQ)\n",
    "        V = torch.matmul(inputs, WV)\n",
    "        \n",
    "        # apply scaled dot-product attention\n",
    "        attention_scores = self.scaled_dot_product(Q, K, V)\n",
    "        \n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f2a08c",
   "metadata": {},
   "source": [
    ">An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. <sub>-from section 3.2 Attention<sub>\n",
    "\n",
    ">In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. <sub>-from section 3.2.1 Scaled Dot-Product Attention<sub>\n",
    "\n",
    "It can be inferred from the paper that K, Q and V weight matrices are distinct but in practice, in the implementation, there is only **one weight matrix** and the projected matrices are also one matrix. The reason for this is to implement the model with highly optimized matrix multiplication code.\n",
    "\n",
    "<img src='assets/1/weight-impl.PNG' />\n",
    "\n",
    "We will not implement this way in this notebook because we want the matrices to be distinguishable so that the code could be easier to follow as much as possible.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a4e87",
   "metadata": {},
   "source": [
    "Let's do a unit test on Attention implementation.\n",
    "\n",
    "<img src='assets/1/attention-matrix.PNG'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba8b0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from section 3.1 Encoder and Decoder Stacks\n",
    "d_model = 512    # embedding dimension\n",
    "\n",
    "# define input\n",
    "X = torch.randn(n, d_model)\n",
    "\n",
    "# init attention \n",
    "attention = Attention(d_model, d_key)\n",
    "\n",
    "# compute attention scores\n",
    "attention_scores = attention(X)\n",
    "\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a877ac",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "<img src='assets/1/multi-head-attention-3.PNG' width=75% height=75%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6efb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention consists of several attention layers running in parallel.       \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head=8, d_model=512, d_key=64, mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.mask = mask\n",
    "        \n",
    "        # number of heads\n",
    "        self.h = n_head\n",
    "        \n",
    "        # add attention layers to a ModuleList container\n",
    "        attention_list = [Attention(d_model, d_key, mask) for _ in range(n_head)]\n",
    "        self.multi_head_attention = nn.ModuleList(attention_list)\n",
    "        \n",
    "        # define linear layer \n",
    "        self.W = nn.Parameter(torch.randn(n_head*d_key, d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # apply & concat attention layers\n",
    "        Z = torch.cat([attention(x) for attention in self.multi_head_attention], -1) \n",
    "        \n",
    "        # apply linear transformation\n",
    "        output = torch.matmul(Z, self.W)    # reduce dimensionality\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e656d",
   "metadata": {},
   "source": [
    "Let's do a unit test on Multi-Head Attention implementation.\n",
    "\n",
    "<img src='assets/1/multi-head-matrix.PNG'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "426095d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from section 3.2.2 Multi-Head Attention\n",
    "h = 8 # number of heads\n",
    "\n",
    "# define input\n",
    "X = torch.randn(n, d_model)\n",
    "\n",
    "# init multi-head attention \n",
    "multi_head_attention = MultiHeadAttention(h, d_model, d_key)\n",
    "\n",
    "# compute multi_head attention score\n",
    "Z = multi_head_attention(X)\n",
    "\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728135e",
   "metadata": {},
   "source": [
    "### Parallelization of Multi-Head Attention in Code\n",
    "\n",
    "Multi-Head attention works with Attention layers running in parallel. However, currently there does  not exist an official implementation in PyTorch for parallel modules. Developers are using third-party libraries in their code to utilize parallelism. Executing the code on GPU avoids sequential execution since underlying execution is asynchronous. Additionally, **if we have multiple GPUs in our system (and don’t use data parallel), we could execute different modules on each device and concatenate the result back on a single device.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a5fb1",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "<img src='assets/1/ffnn.PNG' width=75% height=75%/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb9e29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensionality of input and output is dmodel = 512, \n",
    "# and the inner-layer has dimensionality d_ff = 2048\n",
    "# - from section 3.3 Position-wise Feed-Forward Networks\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model=512, d_feedforward=2048):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        \n",
    "        # define the feedforward neural network \n",
    "        self.feedforwardnn = nn.Sequential(   \n",
    "                nn.Linear(d_model, d_feedforward),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_feedforward, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feedforwardnn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711a7c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test of FeedForward network\n",
    "\n",
    "# from section 3.3 Position-wise Feed-Forward Network\n",
    "d_feedforward = 2048\n",
    "\n",
    "# define input\n",
    "X = torch.rand(n, d_model)\n",
    "\n",
    "# define feedforward neural network\n",
    "ffnn = FeedForwardNetwork(d_model, d_feedforward)\n",
    "\n",
    "# compute ffnn output\n",
    "output = ffnn(X)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17775dc4",
   "metadata": {},
   "source": [
    "## Sublayer\n",
    "\n",
    "\n",
    "<img src='assets/1/sublayer.PNG' width=80% height=80%/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b73441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We employ a residual connection around each of the two sub-layers, \n",
    "# followed by layer normalization. That is'The output of each sub-layer is\n",
    "# LayerNorm(x + Sublayer(x)), where Sublayer(x) is\n",
    "# the function implemented by the sub-layer itself.'\n",
    "# from section - 3.1 Encoder and Decoder Stacks\n",
    "\n",
    "# define residual learning block\n",
    "class Residual_Connection(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super(Residual_Connection, self).__init__()\n",
    "        self.layer = layer     # function implemented by the layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        f_x = self.layer(x)    # apply Sublayer(x)\n",
    "        x = x + f_x            # x + Sublayer(x)\n",
    "        return x\n",
    "    \n",
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, layer, d_model=512):\n",
    "        super(Sublayer, self).__init__()\n",
    "    \n",
    "        self.sublayer = nn.Sequential(\n",
    "                            Residual_Connection(layer), \n",
    "                            nn.LayerNorm(d_model)  # embedding vector length\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sublayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "542b2ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n",
      "torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "# unit test on sublayer\n",
    "\n",
    "# define input\n",
    "X = torch.rand(n, d_model)\n",
    "\n",
    "# init feedforward neural network (with default settings) sublayer\n",
    "nn_layer = Sublayer(FeedForwardNetwork())\n",
    "\n",
    "# compute ffnn sublayer output\n",
    "ffnn_output = nn_layer(X)\n",
    "\n",
    "# init attention sublayer (with default settings)\n",
    "attention_layer = Sublayer(MultiHeadAttention())\n",
    "\n",
    "# compute attention sublayer output\n",
    "attention_output = attention_layer(X)\n",
    "\n",
    "print(ffnn_output.shape)\n",
    "print(attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8013c",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "<img src='assets/1/encoder.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef60f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_head=8, d_model=512, d_key=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(n_head, d_model, d_key)\n",
    "        self.feed_forward = FeedForwardNetwork()\n",
    "        \n",
    "        # two sub-layers\n",
    "        # 1. Self-Attention\n",
    "        self.attention_layer = Sublayer(self.attention)\n",
    "          \n",
    "        # 2. Feedforward Neural Network\n",
    "        self.feedforward_layer = Sublayer(self.feed_forward)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # nn.Sequential(*[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae56124",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "<img src='assets/1/decoder.PNG' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa64735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # three sub-layers\n",
    "        # 1. Feedforward Neural Network\n",
    "        # 2. Self-Attention\n",
    "        # 3. Self-Attention\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2cdba",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "<img src='assets/1/transformer-model.PNG' width=80% height=80%/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44678909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoders are all identical in structure (yet they do not share weights)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    # N is the number of stacked layers of encoders & decoders\n",
    "    def __init__(self, N=6, n_head=8, d_model=512):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # initialize dimensionality of key \n",
    "        d_key = d_model / n_head\n",
    "        \n",
    "        # initialize the encoder and decoder stacks with sequential container\n",
    "        encoders, decoders = [], []\n",
    "        \n",
    "        # stack encoders\n",
    "        for i in range(N):\n",
    "            encoder = Encoder()\n",
    "            encoders.append(encoder)\n",
    "        \n",
    "        # stack decoders\n",
    "        for i in range(N):\n",
    "            decoder = Decoder()\n",
    "            decoders.append(decoder)\n",
    "            \n",
    "        encoders = nn.Sequential(*encoders)\n",
    "        decoders = nn.Sequential(*decoders)\n",
    "        \n",
    "    def forward(self):\n",
    "        # pass through encoders\n",
    "        \n",
    "        # pass through decoders but considering same input\n",
    "        pass\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from section 3.1 Encoder and Decoder Stacks\n",
    "N = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a721f",
   "metadata": {},
   "source": [
    "##  Comparing Results\n",
    "\n",
    "with nn.Trasnformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e245cc2",
   "metadata": {},
   "source": [
    "**Transformers Success**\n",
    "\n",
    "**Transformers Weakness**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a490bc",
   "metadata": {},
   "source": [
    "### Quick Recap of Familiar Components\n",
    "\n",
    "<img src='assets/1/components.PNG'/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09342dd9",
   "metadata": {},
   "source": [
    "### Final Notes\n",
    "\n",
    "The implementation here is a very basic representation of how Transformers work. There are a lot of lacking components that are avoided here to preserve the simplicity of the code. Other than components, in practice, the Transformer model is used with batches with sequence lengths. This introduces new dimensions in the tensors discussed above, and also a lot of new calculations, complex matrix multiplications. There are also a lot of improvements that can be made in the implementation here, and calculations can be further optimized.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537e6bb",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "1. [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Blog Post\n",
    "2. [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) - Paper\n",
    "3. [Transformers for Beginners | What are they and how do they work](https://www.youtube.com/watch?v=_UVfwBqcnbM&t=4s) - Video\n",
    "4. [Implementing a Transformer from Scratch](https://towardsdatascience.com/7-things-you-didnt-know-about-the-transformer-a70d93ced6b2) - Blog Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc30b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
